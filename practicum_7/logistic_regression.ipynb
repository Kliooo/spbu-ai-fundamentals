{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../spbu-ai-fundamentals/config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой теме мы поработаем с данными, посвященными определению рака молочной железы на основе различных признаков анализа клеток в биопсии (радиус, кривизна, симметрия). Известно, что этот датасет линейно разделим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg[\"classification\"][\"wdbc\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Проведите краткий EDA. Есть ли выбросы в данных, какие столбцы коррелируют больше всего, стоит ли преобразоывавть какие-то признаки? Хватит 3-4 графиков или таблиц (но можно больше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Если id есть в данных — удалим\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "# 1. Boxplot для визуализации выбросов\n",
    "plt.figure(figsize=(14, 6))\n",
    "df.select_dtypes(include='number').boxplot(rot=90)\n",
    "plt.title('Boxplot числовых признаков')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Тепловая матрица\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Тепловая матрица признаков')\n",
    "plt.show()\n",
    "\n",
    "# 3. Анализ скошенности\n",
    "skewed = df.select_dtypes(include='number').skew().sort_values(ascending=False)\n",
    "skewed_df = pd.DataFrame(skewed[skewed > 1], columns=[\"Skewness\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=\"Skewness\", \n",
    "    y=skewed_df.index, \n",
    "    data=skewed_df.reset_index(), \n",
    "    hue=\"index\", \n",
    "    palette='viridis',\n",
    "    legend=False\n",
    ")\n",
    "plt.title('Скошенность числовых признаков')\n",
    "plt.xlabel('Skewness')\n",
    "plt.ylabel('Признак')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'] = df['diagnosis'].replace({'B': 0, 'M': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: выведите, сколько в датасете примеров позитивного и негативного класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'diagnosis'\n",
    "\n",
    "target = 'diagnosis'\n",
    "class_counts = df[target].value_counts()\n",
    "\n",
    "print(\"Количество примеров позитивного и негативного класса:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "features.remove('diagnosis')\n",
    "\n",
    "X = df[features]\n",
    "y = df[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить логистическую регрессию на этих данных. Обратите внимание, что по умолчанию применяется L2 регуляризация,мы будем строить предсказания без нее. Однако, в качестве упражнения, сравним результаты с масштабированием признаков и без.\n",
    "\n",
    "**Задание**: оцените, насколько сбалансированы признаки по масштабу. Попробуйте ответить до запуска кода, стоит ли их сначала масштабировать и почему. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Думаю да, стоит. Так как скорее всего не все признаки изначально схожи по масштабу. А обучение модели на таких данных будет не очень качественным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без масштабирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values.reshape(-1), train_size=0.8, shuffle=True)\n",
    "clf = LogisticRegression(penalty=None)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С масштабированием:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values.reshape(-1), train_size=0.8, shuffle=True)\n",
    "clf = LogisticRegression(penalty=None)\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все классификаторы в Sklearn имеют два режима - предсказание лейблов и вероятностей. Предсказание вероятностей дает нам необработанные оценки принадлежности к тому или иному классу. Модель в таком случае возвращает вектор (для каждого семпла) размера N (где N - число классов). \n",
    "\n",
    "**Вопрос**: Какого размера будет предсказание в случае бинарной логистической регрессии? А многоклассовой? Другими словами, в каких случаях негативный класс добавляется как отдельный?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной размерность будет nx2 и естетсвенно 2 класса. В случае многоклассновой nxN, где N - кол-во классов. Негативный класс добавляется как отдельный всегда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    'pred': clf.predict(X_test).reshape(-1),\n",
    "    'pred_proba': clf.predict_proba(X_test)[:, 1],\n",
    "    'true': y_test.reshape(-1),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Постройте матрицу предсказаний 100x2 для регрессии с двумя классами, где в каждой строке будут случайные значения. \n",
    "1) Получите из этого оценку принадлежности к классу с помощью сигмоиды и софтмакса. \n",
    "2) Постройте предсказание класса. В случае сигмоиды предсказывайте принадлежность к классу на основе границы, софтмакса - по максимальной вероятности\n",
    "\n",
    "**Вопрос***: как еще можно предсказать класс? Всегда ли нужно брать именно эти функции?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_matrix = np.random.randn(100, 2)\n",
    "\n",
    "# Функция сигмоиды для преобразования значений в вероятности\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Функция софтмакса для нормализации значений по строкам\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "sigmoid_probs = sigmoid(pred_matrix)\n",
    "softmax_probs = softmax(pred_matrix)\n",
    "\n",
    "print(\"Вероятности после применения сигмоиды:\\n\")\n",
    "print(sigmoid_probs[:5], \"\\n\")\n",
    "\n",
    "print(\"Вероятности после применения софтмакса:\\n\")\n",
    "print(softmax_probs[:5], \"\\n\")\n",
    "\n",
    "sigmoid_pred_class = (sigmoid_probs[:, 0] >= 0.5).astype(int)\n",
    "\n",
    "softmax_pred_class = np.argmax(softmax_probs, axis=1)\n",
    "\n",
    "#print(\"Предсказания классов по сигмоиде (используя порог 0.5):\")\n",
    "#print(sigmoid_pred_class)\n",
    "#print(\"Предсказания классов по софтмаксу (на основе максимальной вероятности):\")\n",
    "#print(softmax_pred_class)\n",
    "\n",
    "unique_sigmoid, counts_sigmoid = np.unique(sigmoid_pred_class, return_counts=True)\n",
    "unique_softmax, counts_softmax = np.unique(softmax_pred_class, return_counts=True)\n",
    "\n",
    "print(\"Распределение классов по сигмоиде:\\n\")\n",
    "for cls, count in zip(unique_sigmoid, counts_sigmoid):\n",
    "    print(f\"Класс {cls}: {count} строк\")\n",
    "    \n",
    "print(\"\\nРаспределение классов по софтмаксу:\\n\")\n",
    "for cls, count in zip(unique_softmax, counts_softmax):\n",
    "    print(f\"Класс {cls}: {count} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики классификации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики на основе лейблов\n",
    "Рассмотрим, какие у нас могут быть тезультаты классификации.\n",
    "\n",
    "* TP (true positive) - правильно предсказали: рак есть, что модель и предсказала\n",
    "* FP (false positive) - неправильно предсказали: рака нет,  а модель предсказала, что есть (1st order error)\n",
    "* FN (false negative) - неправильно предсказали: рак вообще-то есть,  а модель предсказала, что нет (2nd order error)!\n",
    "* TN (true negative) - правильно предсказали: рака нет, что модель и предсказала\n",
    "\n",
    "\n",
    "Pos/Neg - общее количество объектов класса 1/0\n",
    "\n",
    "Метрики:\n",
    "\n",
    "* $ \\text{Accuracy} = \\frac{TP + TN}{Pos+Neg}$ - Доля правильных ответов\n",
    "* $ \\text{Error rate} = 1 -\\text{accuracy}$ - Доля ошибок\n",
    "* $ \\text{Precision} =\\frac{TP}{TP + FP}$ - Точность\n",
    "* $ \\text{Recall} =\\frac{TP}{TP + FN} = \\frac{TP}{Pos}$ - Полнота\n",
    "* $ \\text{F}_\\beta \\text{-score} = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}$ F-мера (часто используют F1-меру, где $\\beta=1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC кривая\n",
    "\n",
    "ROC кривая измеряет насколько хорошо классификатор разделяет два класса. Она построена на предсказании вероятности. Площадь под ней (ROC-AUC) является неплохой оценкой общего качества предсказаний. \n",
    " \n",
    "Пусть $y_{\\rm i}$ - истинная метрка и $\\hat{y}_{\\rm i}$ - прогноз вероятности для $i^{\\rm th}$ объекта.\n",
    "\n",
    "Число положительных и отрицательных объектов: $\\mathcal{I}_{\\rm 1} = \\{i: y_{\\rm i}=1\\}$ and $\\mathcal{I}_{\\rm 0} = \\{i: y_{\\rm i}=0\\}$.\n",
    "\n",
    "Для каждого порогового значения вероятности $\\tau$ считаем True Positive Rate (TPR) и False Positive Rate (FPR):\n",
    "\n",
    "\\begin{equation}\n",
    "TPR(\\tau) = \\frac{1}{I_{\\rm 1}} \\sum_{i \\in \\mathcal{I}_{\\rm 1}} I[\\hat{y}_{\\rm i} \\ge \\tau] = \\frac{TP(\\tau)}{TP(\\tau) + FN(\\tau)} = \\frac{TP(\\tau)}{Pos}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "FPR(\\tau) = \\frac{1}{I_{\\rm 0}} \\sum_{i \\in \\mathcal{I}_{\\rm 0}} I[\\hat{y}_{\\rm i} \\ge \\tau]= \\frac{FP(\\tau)}{FP(\\tau) + TN(\\tau)} = \\frac{FP(\\tau)}{Neg}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=10000, n_features=10, n_informative=5, n_redundant=5, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем для сравнения случайный предикт. Иногда это не худшая стратегия. Если в данных мало сигнала, случайное предсказание может работать лучше ложного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "random_classifier = DummyClassifier(strategy='uniform', random_state=42).fit(X_train, y_train)\n",
    "y_random = random_classifier.predict_proba(X_test)[:,1]\n",
    "y_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_preds = random_classifier.predict(X_test)\n",
    "random_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "def depict_pr_roc(y_true, y_pred, classifier_name='Some Classifier', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(11, 5))\n",
    "\n",
    "    print(classifier_name, 'metrics')\n",
    "    PrecisionRecallDisplay.from_predictions(y_true, y_pred, ax=ax[0], name=classifier_name)\n",
    "    print('AUC-PR: %.4f' % average_precision_score(y_true, y_pred))\n",
    "    ax[0].set_title(\"PRC\")\n",
    "    ax[0].set_ylim(0, 1.1)\n",
    "\n",
    "    RocCurveDisplay.from_predictions(y_true, y_pred, ax=ax[1], name=classifier_name)\n",
    "    print('AUC-ROC: %.4f' % roc_auc_score(y_true, y_pred))\n",
    "    ax[1].set_title(\"ROC\")\n",
    "    ax[1].set_ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "depict_pr_roc(y_test, y_random, 'Random Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также посчитаем другие метрики на основе лейблов.\n",
    "\n",
    "**Задание:** Дополните код по рассчету метрик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def quality_metrics_report(y_true, y_pred):\n",
    "    \n",
    "    tp = np.sum( (y_true == 1) & (y_pred == 1) )\n",
    "    fp = np.sum( (y_true == 0) & (y_pred == 1) )\n",
    "    fn = np.sum( (y_true == 1) & (y_pred == 0) )\n",
    "    tn = np.sum( (y_true == 0) & (y_pred == 0) )\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    error_rate = 1 - accuracy\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return [tp, fp, fn, tn, accuracy, error_rate, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe для сравнения методов классификации по метрикам\n",
    "df_metrics = pd.DataFrame(\n",
    "    columns=['acc', 'er', 'precision', 'recall', 'f1', 'auc_pr', 'roc_auc_score', 'reg_const']\n",
    ")\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_random)\n",
    "\n",
    "# добавление очередной строки с характеристиками метода\n",
    "[tp, fp, fn, tn, accuracy, error_rate, precision, recall, f1] = quality_metrics_report(y_test, random_preds)\n",
    "df_metrics.loc['Random Classifier'] = [\n",
    "      accuracy, error_rate, precision, recall, f1,\n",
    "      average_precision_score(y_test, y_random),\n",
    "      roc_auc_score(y_test, y_random),\n",
    "      0,\n",
    "]\n",
    "\n",
    "# по аналогии результаты следующих экспериментов можно будет собрать в табличку\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение логистической регрессии\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Получение предсказаний\n",
    "y_pred_logreg = clf.predict(X_test)\n",
    "y_pred_prob_logreg = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob_logreg) # Получение метрик для логистической регрессии\n",
    "\n",
    "[tp, fp, fn, tn, accuracy, error_rate, precision_val, recall_val, f1] = quality_metrics_report(y_test, y_pred_logreg)\n",
    "df_metrics.loc['Logistic Regression'] = [\n",
    "    accuracy, error_rate, precision_val, recall_val, f1,\n",
    "    average_precision_score(y_test, y_pred_prob_logreg),\n",
    "    roc_auc_score(y_test, y_pred_prob_logreg),\n",
    "    0,\n",
    "]\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласуются ли метрики? В чем может быть проблема accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по выводу, Logistic Regression имеет большую точность (acc) и меньшую ошибку (er). Возможно проблема с accuracy может быть в случае несбалансированных данных, если класс 0 встречается сильно чаще, и модель просто предсказывает его, получая высокую точность, но не решая задачу правильно. Думаю если такое произошло важнее смотреть на precision, recall, f1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Соберите табличку для разных классификаторов.\n",
    "\n",
    "**Задание**: Постройте график PR-curve, ROC-curve для лучшего из них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Случайный лес\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "y_pred_prob_rf = clf_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob_rf)\n",
    "\n",
    "[tp, fp, fn, tn, accuracy, error_rate, precision_val, recall_val, f1] = quality_metrics_report(y_test, y_pred_rf)\n",
    "df_metrics.loc['Random Forest'] = [\n",
    "    accuracy, error_rate, precision_val, recall_val, f1,\n",
    "    average_precision_score(y_test, y_pred_prob_rf),\n",
    "    roc_auc_score(y_test, y_pred_prob_rf),\n",
    "    0,\n",
    "]\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svc = SVC(probability=True)\n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svc = clf_svc.predict(X_test)\n",
    "y_pred_prob_svc = clf_svc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob_svc)\n",
    "\n",
    "[tp, fp, fn, tn, accuracy, error_rate, precision_val, recall_val, f1] = quality_metrics_report(y_test, y_pred_svc)\n",
    "df_metrics.loc['SVM'] = [\n",
    "    accuracy, error_rate, precision_val, recall_val, f1,\n",
    "    average_precision_score(y_test, y_pred_prob_svc),\n",
    "    roc_auc_score(y_test, y_pred_prob_svc),\n",
    "    0,\n",
    "]\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = clf_knn.predict(X_test)\n",
    "y_pred_prob_knn = clf_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob_knn)\n",
    "\n",
    "[tp, fp, fn, tn, accuracy, error_rate, precision_val, recall_val, f1] = quality_metrics_report(y_test, y_pred_knn)\n",
    "df_metrics.loc['KNN'] = [\n",
    "    accuracy, error_rate, precision_val, recall_val, f1,\n",
    "    average_precision_score(y_test, y_pred_prob_knn),\n",
    "    roc_auc_score(y_test, y_pred_prob_knn),\n",
    "    0,\n",
    "]\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "# Предсказания для k-NN\n",
    "precision_knn, recall_knn, _ = precision_recall_curve(y_test, y_pred_prob_knn)\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_prob_knn)\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# PR-curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(recall_knn, precision_knn, color='b', label='PR-curve (k-NN)')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "# ROC-curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_knn, tpr_knn, color='b', label='ROC-curve (k-NN)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve (AUC = {roc_auc_knn:.2f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Постройте таблицу точности для набора данных wbdc. Сделайте по таблице метрик на обучающей и тестовой выборках. В таблице сравните разные преобразования признаков и гиперпараметры (регуляризацию). Можно сделать три-четыре эксперимента. \n",
    "- На каком эксперименте получилось достичь лучшего качества на трейне?\n",
    "- А на тесте?\n",
    "- Переобучается ли модель?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop(columns=[\"diagnosis\"])\n",
    "y = df[\"diagnosis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scalers = {\n",
    "    \"None\": None,\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler()\n",
    "}\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10]\n",
    "results = []\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    for C in C_values:\n",
    "        X_tr = X_train.copy()\n",
    "        X_te = X_test.copy()\n",
    "\n",
    "        if scaler:\n",
    "            scaler_instance = scaler\n",
    "            X_tr = scaler_instance.fit_transform(X_tr)\n",
    "            X_te = scaler_instance.transform(X_te)\n",
    "\n",
    "        model = LogisticRegression(C=C, max_iter=10000)\n",
    "        model.fit(X_tr, y_train)\n",
    "\n",
    "        # Оценка\n",
    "        acc_train = accuracy_score(y_train, model.predict(X_tr))\n",
    "        acc_test = accuracy_score(y_test, model.predict(X_te))\n",
    "\n",
    "        results.append({\n",
    "            \"Scaler\": scaler_name,\n",
    "            \"C\": C,\n",
    "            \"Train Accuracy\": acc_train,\n",
    "            \"Test Accuracy\": acc_test\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df[\"Overfitting Gap\"] = results_df[\"Train Accuracy\"] - results_df[\"Test Accuracy\"]\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "best_train = results_df.loc[results_df[\"Train Accuracy\"].idxmax()]\n",
    "best_test = results_df.loc[results_df[\"Test Accuracy\"].idxmax()]\n",
    "\n",
    "print(f\"\\nЛучшая точность на трейне: Scaler={best_train['Scaler']}, C={best_train['C']}, Точность: {best_train['Train Accuracy']:.6f}\")\n",
    "print(f\"Лучшая точность на тесте: Scaler={best_test['Scaler']}, C={best_test['C']}, Точность: {best_test['Test Accuracy']:.6f}\")\n",
    "\n",
    "print(\"\\nGap между train и test (чем больше, тем вероятнее переобучение):\")\n",
    "print(results_df[[\"Scaler\", \"C\", \"Overfitting Gap\"]].sort_values(by=\"Overfitting Gap\", ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
