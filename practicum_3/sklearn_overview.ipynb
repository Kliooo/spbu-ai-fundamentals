{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интерфейсы scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "\n",
    "Для примера построим простой estimator, который в перспективе будет вычитать из признаков их среднее значение и после сдвигать признаки на заранее заданную константу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtractMeanAndShiftEstimator(BaseEstimator):\n",
    "    def __init__(self, shift=0.):\n",
    "        self.shift: float = shift\n",
    "        self.means_: NDArray = None  # we add a trailing underscore for parameters which will be learnt in fit()\n",
    "\n",
    "    def fit(self, X: NDArray, y: NDArray = None):\n",
    "        # y is ignored here\n",
    "        self.means_ = X.mean(axis=0)  # the first axis corresponds to samples by default\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SubtractMeanAndShiftEstimator(shift=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `get_params()` реализован в `BaseEstimator`, и мы можем сразу использовать его для получения гиперпараметров модели. Это возможно, так как единственный гиперпараметр `shift` был передан как явное ключевое слово в контрукторе\n",
    "\n",
    "Обратите внимание, что соответствующий аттрибут класса должен совпадать с ключевым словом: `self.shift = shift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично мы можем использовать `set_params()` для задания значений гиперпараметров. Этот метод пригодится при поиске оптимальных значений гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.set_params(shift=5)\n",
    "m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 10],\n",
    "    [3, 30],\n",
    "    [2, 20],\n",
    "])\n",
    "y = np.array([\n",
    "    [ 0, -8],\n",
    "    [ 2, 10],\n",
    "    [ 1,  1],\n",
    "])\n",
    "m.fit(X, y)\n",
    "print(m.means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В sklearn есть класс sklearn.base.OutlierMixin, который позволяет реализовывать кастомные классы для определения выбросов.\n",
    "Он добавляет:\n",
    "- атрибут _estimator_type, по умолчанию outlier_detector\n",
    "- fit_predict.\n",
    "\n",
    "Метод fit() работает в формате без учителя, predict же должен классифицировать данные на аутлаеры (возвращать для них -1) и обыычные данные (возвращать 1). Для классификации используется отсечка по порогу предсказаний, полученных внутренним.\n",
    "Во встроенных методах функция оценки доступна с помощью метода `score_samples`, в то время как порог можно задать параметром `contamination`. \n",
    "Например, для гауссовских данных можно использовать sklearn.covariance.EllipticEnvelope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Создайте свой эстиматор с использованием sklearn.base.OutlierMixin, который будет определять выбросы на основе интерквартильного размаха. \n",
    "Он должен возвращать один столбец с 1 и -1, а также позволять задавать порог для квантиля, определяющего размах. Не забудьте, что он должен быть двухсторонним.\n",
    "Ваш эстиматор должен работать и для датафреймов, и для numpy массивов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, OutlierMixin\n",
    "\n",
    "class MyEstimator(BaseEstimator, OutlierMixin):\n",
    "    def __init__(self, quantile_range=1.5):\n",
    "        self.quantile_range = quantile_range\n",
    "        self.lower_bound_ = None\n",
    "        self.upper_bound_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self._convert_to_array(X)\n",
    "        Q1 = np.percentile(X, 25, axis=0)\n",
    "        Q3 = np.percentile(X, 75, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        self.lower_bound_ = Q1 - self.quantile_range * IQR\n",
    "        self.upper_bound_ = Q3 + self.quantile_range * IQR\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._convert_to_array(X)\n",
    "        is_outlier = (X < self.lower_bound_) | (X > self.upper_bound_)\n",
    "        return np.where(is_outlier.any(axis=1), -1, 1)\n",
    "\n",
    "    def _convert_to_array(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.to_numpy()\n",
    "        return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [10, 200],\n",
    "    [15, 220],\n",
    "    [50, 10000],\n",
    "    [12, 205],\n",
    "    [14, 210],\n",
    "    [100, 4000]\n",
    "])\n",
    "\n",
    "detector = MyEstimator(quantile_range=1.5)\n",
    "detector.fit(X)\n",
    "\n",
    "preds = detector.predict(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor\n",
    "\n",
    "Рассмотрим тот же класс, но добавим к нему методы `predict()` и `score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtractMeanAndShiftPredictor(BaseEstimator):\n",
    "    def __init__(self, shift=0.):\n",
    "        self.shift: float = shift\n",
    "        self.means_: NDArray = None  # we add a trailing underscore for parameters which will be learnt in fit()\n",
    "\n",
    "    def fit(self, X: NDArray, y: NDArray = None):\n",
    "        # y is ignored here\n",
    "        self.means_ = X.mean(axis=0)  # the first axis corresponds to samples by default\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: NDArray) -> NDArray:\n",
    "        e = np.ones((X.shape[0], 1))\n",
    "        return X -  e @ self.means_.reshape(-1, 1).T + self.shift\n",
    "\n",
    "    def score(self, X: NDArray, y: NDArray) -> float:\n",
    "        return r2_score(y, self.predict(X))  # R2 \\in (-\\infty; 1] is the coefficient of determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы специально добавили небольшое отклонение в y, наш R2 чуть меньше 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SubtractMeanAndShiftPredictor(shift=1)\n",
    "model.fit(X)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Рассмотрим тот же класс, но добавим к нему метод `transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtractMeanAndShiftTransformer(BaseEstimator, OneToOneFeatureMixin, TransformerMixin):\n",
    "    def __init__(self, shift=0.):\n",
    "        self.shift: float = shift\n",
    "        self.means_: NDArray = None  # we add a trailing underscore for parameters which will be learnt in fit()\n",
    "\n",
    "    def fit(self, X: NDArray, y: NDArray = None):\n",
    "        # y is ignored here\n",
    "        self.means_ = X.mean(axis=0)  # the first axis corresponds to samples by default\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: NDArray) -> NDArray:\n",
    "        e = np.ones((X.shape[0], 1))\n",
    "        return X -  e @ self.means_.reshape(-1, 1).T + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = SubtractMeanAndShiftTransformer(shift=5)\n",
    "t.fit(X)\n",
    "t.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы добавили `TransformerMixin`, мы можем использовать метод `fit_transform()`, не реализуя его явно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично мы можем использовать метод `get_feature_names_out()`, так как мы добавили `OneToOneFeatureMixin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.get_feature_names_out(input_features=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем датасет с домами как пример. Вспомним, что мы делали в прошлый раз, и попробуем заполнить пропущенные значения в некоторых числовызх столбцах.\n",
    "Для этого используем трансформер по столбцам. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg['house_pricing']['train_dataset'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию, только указанные столбцы трансформируются и возвращаются (remainder=`drop`). Мы же сделаем так, чтобы все остальные столбцы тоже возвращались, просто с ними бы ничего не делалось. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [('mean_impute', SimpleImputer(strategy='mean'), ['SalePrice', 'LotArea', 'WoodDeckSF',  'MasVnrArea'])], \n",
    "    remainder=\"passthrough\", force_int_remainder_cols=False)\n",
    "\n",
    "ct.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у датасета появятсся столбцы, которые не были представлены во время fit (даже среди тех, что не трансформировались), то они будут выкинуты на этапе transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"temp\"] = 0\n",
    "ct.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Перейдите к медианному заполнению пропусков. Проверьте, что результаты, полученные с помощью трансформации, соответствуют преобразованию напрямую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg['house_pricing']['train_dataset'])\n",
    "df.head()\n",
    "\n",
    "ct_median = ColumnTransformer(\n",
    "    [('median_impute', SimpleImputer(strategy='median'), ['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea'])], \n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "ct_median.fit(df)\n",
    "df_transformed_ct = ct_median.transform(df)\n",
    "\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df_direct_impute = df.copy()\n",
    "\n",
    "df_direct_impute[['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']] = imputer_median.fit_transform(df_direct_impute[['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']])\n",
    "\n",
    "comparison = (df_transformed_ct[:, :4] == df_direct_impute[['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']].to_numpy())\n",
    "\n",
    "print(comparison.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Добавьте еще нормализатор для LotFrontage, LotArea и запустите в ColumnTransformer. Обучите его и примените."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('median_impute', SimpleImputer(strategy='median'), ['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']),\n",
    "     ('normalize', StandardScaler(), ['LotFrontage', 'LotArea'])],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=True\n",
    ")\n",
    "\n",
    "ct.set_output(transform='pandas')\n",
    "df_transformed = ct.fit_transform(df)\n",
    "\n",
    "df_direct = df.copy()\n",
    "\n",
    "df_direct[['median_impute__SalePrice', 'median_impute__LotArea', 'median_impute__WoodDeckSF', 'median_impute__MasVnrArea']] = \\\n",
    "df_direct[['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']].fillna(df[['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']].median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_direct[['normalize__LotFrontage', 'normalize__LotArea']] = scaler.fit_transform(df_direct[['LotFrontage', 'LotArea']])\n",
    "\n",
    "changed_columns = ['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea', 'LotFrontage']\n",
    "remaining_columns = [col for col in df.columns if col not in changed_columns]\n",
    "df_direct[[f'remainder__{col}' for col in remaining_columns]] = df_direct[remaining_columns]\n",
    "\n",
    "df_direct = df_direct[[f\"median_impute__{col}\" for col in ['SalePrice', 'LotArea', 'WoodDeckSF', 'MasVnrArea']] + \n",
    "                      [f\"normalize__{col}\" for col in [\"LotFrontage\", \"LotArea\"]] +\n",
    "                      [f\"remainder__{col}\" for col in remaining_columns]]\n",
    "\n",
    "print(\"Индексы совпали:\", df_transformed.index.equals(df_direct.index))\n",
    "print(\"Колонки совпали:\", df_transformed.columns.equals(df_direct.columns))\n",
    "print(\"Совпали полностью:\", df_transformed.equals(df_direct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn располагает большим количеством встроенных трансформеров. Соответствующие трансформеры есть и для категориальных фичей (более подробно рассмотрим этот тип чуть позже). Например, известное нам бинарное кодирование можно проводить с помощью OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('median_impute', SimpleImputer(strategy='mean'), ['SalePrice', 'LotArea', 'WoodDeckSF',  'MasVnrArea']),\n",
    "        (\"one_hot_encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"MSZoning\", \"SaleType\", \"SaleCondition\"]),\n",
    "    ], \n",
    "    remainder=\"passthrough\", force_int_remainder_cols=False)\n",
    "\n",
    "ct.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.set_output(transform='pandas')\n",
    "ct.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выбора столбцов можно создавать make_selector, например, по выбору численных и категориальных значений. \n",
    "\n",
    "**Доп. задание**. Сделайте трансформер для OneHotEncoder на основе make_selector так, чтобы выбирать все нечисловые столбцы. Сколько столбцов получается после трансформации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "non_numeric_columns = make_column_selector(dtype_include='object')(df)\n",
    "\n",
    "print(f\"Количество нечисловых столбцов до трансформации: {len(non_numeric_columns)}\")\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False), make_column_selector(dtype_include='object'))\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "df_transformed = ct.fit_transform(df)\n",
    "\n",
    "df_transformed = pd.DataFrame(df_transformed)\n",
    "\n",
    "num_columns_after_transform = df_transformed.shape[1]\n",
    "\n",
    "print(f\"Количество столбцов после трансформации: {num_columns_after_transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "С помощью Pipeline мы можем производить последовательную обработку данных и выполнять предсказание в конце"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 10],\n",
    "    [3, 30],\n",
    "    [2, 20],\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [2],\n",
    "    [1],\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"shifter\", SubtractMeanAndShiftTransformer(shift=5)),\n",
    "    (\"regressor\", LinearRegression()),\n",
    "])\n",
    "...\n",
    "pipeline.fit(X, y)\n",
    "y_pred = pipeline.predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline хранит последовательные Estimators в аттрибуте `steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейти к объекту i-го Estimator можно напрямую через `pipeline[i]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline[1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как Pipeline сам является Estimator, мы можем увидеть список его параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, параметры промежуточных Estimator указаны как `<estimator>__<parameter>`. Следовательно, мы можем изменить параметры любого промежуточного Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_params(shifter__shift=10)\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Создайте пайплайн по преобразованию чсиленных столбцов, содержащий импьютер и скейлер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 10, np.nan],\n",
    "    [3, 30, 3],\n",
    "    [2, 20, 2],\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline.fit(X)\n",
    "\n",
    "X_transformed = pipeline.transform(X)\n",
    "\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Создайте новый трансформер, который для категориальных столбцов будет заполнять пропущенные значения наиболее часто встречаюмшщимся или новой категорией (сделайте параметром). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "X = np.array([\n",
    "    ['cat', 'dog', np.nan],\n",
    "    ['dog', np.nan, 'apple'],\n",
    "    ['cat', 'dog', 'banana'],\n",
    "], dtype=object)\n",
    "\n",
    "def impute_with_new_category(X, new_category='Missing'):\n",
    "    return np.where(pd.isnull(X), new_category, X)\n",
    "\n",
    "categorical_columns = [0, 1, 2]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(transformers=[('categorical', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('new_category', FunctionTransformer(lambda X: impute_with_new_category(X, 'NewCategory')))]), categorical_columns)]))\n",
    "])\n",
    "\n",
    "pipeline.fit(X)\n",
    "X_transformed = pipeline.transform(X)\n",
    "\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Создайте пайплайн по преобразованию категориальных столбцов, содержащий ваш импьютер и OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    ['cat', 'dog', np.nan],\n",
    "    ['dog', np.nan, 'apple'],\n",
    "    ['cat', 'dog', 'banana'],\n",
    "], dtype=object)\n",
    "\n",
    "categorical_columns = [0, 1, 2]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(transformers=[('categorical', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_columns)]))\n",
    "])\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Создайте ColumnTransformer, который будет содержать в себе два вышеуказанных пайплайна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    ['cat', 'dog', np.nan, 5.0],\n",
    "    ['dog', np.nan, 'apple', 7.0],\n",
    "    ['cat', 'dog', 'banana', np.nan],\n",
    "], dtype=object)\n",
    "\n",
    "categorical_columns = [0, 1, 2]\n",
    "numerical_columns = [3]\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('categorical', categorical_pipeline, categorical_columns),\n",
    "    ('numerical', numerical_pipeline, numerical_columns)]\n",
    ")\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Доп.задание**: Используйте для комбинации результатов двух отдельных трансформеров FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "X = np.array([\n",
    "    ['cat', 'dog', np.nan, 5.0],\n",
    "    ['dog', np.nan, 'apple', 7.0],\n",
    "    ['cat', 'dog', 'banana', np.nan],\n",
    "], dtype=object)\n",
    "\n",
    "categorical_columns = [0, 1, 2]\n",
    "numerical_columns = [3]\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', categorical_pipeline, categorical_columns),\n",
    "        ('numerical', numerical_pipeline, numerical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Дополнительный трансформер: вычисляем длину строковых значений в категориальных столбцах\n",
    "def string_lengths(X):\n",
    "    return np.array([[len(str(x)) for x in row] for row in X[:, categorical_columns]])\n",
    "\n",
    "string_length_transformer = FunctionTransformer(string_lengths)\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('string_lengths', string_length_transformer)\n",
    "])\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('feature_union', feature_union),\n",
    "])\n",
    "\n",
    "X_transformed = final_pipeline.fit_transform(X)\n",
    "\n",
    "print(X_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
