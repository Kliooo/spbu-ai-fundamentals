{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contemporary-rouge",
   "metadata": {},
   "source": [
    "\n",
    "# Случайный лес\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d7ea4-7669-4cba-93ab-7bbf6a2e670d",
   "metadata": {},
   "source": [
    "В данном случае мы говорим об **ансамбле** решающих деревьев (отсюда и слово лес). \n",
    "В обычных деревьях для задачи классификации в листях деревьев лежит класс. Для задачи регресии в листях лежит среднее значение ответов для всех объектов. \n",
    "Чтобы получить результат работы леса, ответы отдельных деревьев должны быть как-то аггрегированы. Вопрос: Как это можно удобно сделать?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e64f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.201889Z",
     "start_time": "2023-05-10T11:49:30.519955Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import trange\n",
    "import category_encoders as ce\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-recorder",
   "metadata": {},
   "source": [
    "## Решающее дерево\n",
    "Начнём с построения решающего дерева. Для демонстрации используем упрощенный датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-willow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.346252Z",
     "start_time": "2023-05-10T11:49:31.338141Z"
    }
   },
   "outputs": [],
   "source": [
    "california = fetch_california_housing()\n",
    "california_X = pd.DataFrame(data=california.data, columns=california.feature_names)\n",
    "california_Y = california.target\n",
    "print(f\"X shape: {california_X.shape}, Y shape: {california_Y.shape}\")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    california_X, california_Y, test_size=0.3, random_state=123, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58dbce-0101-477f-aec6-87a4b90a156b",
   "metadata": {},
   "source": [
    "**Задание**: Ниже пример подготовки нашего датасета с домами. Проведите все действия этого ноутбука и для него. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eeaf67d-2564-4976-bfc6-e4ebb49efa12",
   "metadata": {},
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b101319e-5516-40a3-a434-49e157f4b44f",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv(cfg['house_pricing']['train_dataset'])\n",
    "train_df.head()\n",
    "train_df[\"Exterior2nd\"] = train_df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n",
    "    # Some values of GarageYrBlt are corrupt, so we'll replace them\n",
    "    # with the year the house was built\n",
    "train_df[\"GarageYrBlt\"] = train_df[\"GarageYrBlt\"].where(train_df.GarageYrBlt <= 2010, train_df.YearBuilt)\n",
    "    # Names beginning with numbers are awkward to work with\n",
    "train_df.rename(columns={\n",
    "        \"1stFlrSF\": \"FirstFlrSF\",\n",
    "        \"2ndFlrSF\": \"SecondFlrSF\",\n",
    "        \"3SsnPorch\": \"Threeseasonporch\",\n",
    "        }, inplace=True,)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df.drop(\"SalePrice\", axis=1),train_df[\"SalePrice\"],  test_size=0.3, random_state=123, shuffle=True\n",
    ")\n",
    "\n",
    "cat_сols = X_train.select_dtypes(include=['object']).columns\n",
    "count_encoder = ce.CountEncoder()\n",
    "X_train = count_encoder.fit_transform(X_train, y_train)\n",
    "X_test = count_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-jacksonville",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:31.459233Z",
     "start_time": "2023-05-10T11:49:31.346360Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# TODO: обучите решающее дерево без ограничений на тренировочной выборке\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
    "print(f\"MSE on train set: {mean_squared_error(y_train, tree.predict(X_train)):.2f}\")\n",
    "print(f\"MSE on test set: {mean_squared_error(y_test, tree.predict(X_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11287a4d-bd15-4ada-a9fe-3919f54251fa",
   "metadata": {},
   "source": [
    "Мы видим, что наше дерево сильно переобучилось - оно идеально предсказывает значения на трейне и довольно плохо - на тесте. Заглянем в эту проблему чуть глубже. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa34e9-4180-4f8b-8415-7ba3820d30dd",
   "metadata": {},
   "source": [
    "Среднеквадратичный риск на фиксированной выборке X можно расписать как \n",
    "$$E = Var(h) + Bias^2(h) + Noise(y)$$\n",
    "Здесь $Bias^2(h) = E_x[(\\overline{h}(X) - \\overline{y}(X))^2]$ показывает, насколько средняя модель отклонится от матожидания таргета (идеальной модели). \n",
    "$Var(h) = E_{x,D}[(h(X, D) - \\overline{h}(X))^2]$ - показывает разброс обученных моделей относительно среднего ответа. \n",
    "$Noise(y) = E_{x,y}[(\\overline{y}(X) - Y)^2]$ - дисперсия самого таргета при фиксированном x. Это неустранимая ошибка, которой соответствует самый идеальный прогноз.\n",
    "Описание, как оно выводится, есть [здесь](https://getsomemath.ru/) и [здесь](https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition). \n",
    "\n",
    "Смещение показывает, насколько хорошо можно с помощью данного семейства моделей приблизиться к оптимальной модели. Как правило, оно маленькое у сложных семейств и большое у относительно простых. Вопрос: Назовите такие семейства.\n",
    "\n",
    "Дисперсия показывает, насколько будет изменяться предсказание в зависимости от выборки - то есть насколько ваше семейство склонно к переобучению. \n",
    "\n",
    "Существует известный пример сочетания смещения и разброса:\n",
    "\n",
    "![bv](../additional/images/bv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271e890-842b-4592-a8bb-31f0e673f7e4",
   "metadata": {},
   "source": [
    "На нашем примере с домами посмотрим, какие bias и variance имеет наше дерево. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-yemen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:49:47.314276Z",
     "start_time": "2023-05-10T11:49:31.461093Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке\n",
    "avg_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "    tree,\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    np.array(X_test),\n",
    "    np.array(y_test),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "print(f\"Average test loss: {avg_loss:.2f}\")\n",
    "print(f\"Average test bias: {avg_bias:.2f}\")\n",
    "print(f\"Average test variance: {avg_var:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee6365-05e0-4628-8a98-8b6943f729dd",
   "metadata": {},
   "source": [
    "Декомпозиция довольно долгая, так как для вычисленя средних мы много раз (по умолчанию, 200) строим подвыборки тестового сета. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-murder",
   "metadata": {},
   "source": [
    "Мы видим, что у нашего дерева высокая дисперсия и низкое смещение. Постараемся исправить это. Один из способов борьбы с переобучением – построение композиций моделей. На этом семинаре мы рассмотрим построение композиций при помощи бэггинга."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc9974-47ea-457e-be34-572d537087cd",
   "metadata": {},
   "source": [
    "**Задание:**  Постройте графики зависимости смещения и разброса относительно глубины дерева, обученного на этом датасете.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-password",
   "metadata": {},
   "source": [
    "### Бэггинг\n",
    "Бэггинг основан на идее бутстрапа. Бутстрап работает на удивление просто.\n",
    "Предположим, что наша выборка D размера n на представляет генеральную совокупность. После этого мы можем сгененрировать эмпирическое распределение необходимой статистики, выбирая с замещением $N >> 100$ подвыборок объема n из этой совокупности (назовем псевдовыборками), и рассчитывая для них нужную статистику. \n",
    "![bag](../additional/images/bootstrap.png)\n",
    "\n",
    "\n",
    "Суть алгоритма бэггинга:\n",
    "1. Обучаем много деревьев на бутстрапированных подвыборках исходной выборки независимо друг от друга. Бутстрапированную подвыборку строим при помощи выбора $N$ (размер исходной выборки) наблюдений из исходной выборки с возвращением.\n",
    "2. Усредняем предсказания всех моделей (например, берём арифметическое среднее).\n",
    "\n",
    "Можно показать, что модель, построенная при помощи бэггинга, будет иметь **то же смещение**, что и у отдельных деревьев, но значительно **меньшую дисперсию** (если деревья нескоррелированы - именно этогно мы пытаемся достичь в RF, семплируя фичи).  В идеальном случае дисперсия уменьшится в N раз! На практике же мы такого вряд ли добьемся, но всек же улучшения ожидать стоит. \n",
    "\n",
    "Для начала обучим просто беггинг - он тоже есть в sklearn, и в него можно передать любые базовые модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-pacific",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:50:53.873857Z",
     "start_time": "2023-05-10T11:49:47.316082Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_tree = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "# TODO: обучите бэггинг с 20 деревьями, каждое из которых строится без ограничений\n",
    "bag = BaggingRegressor(base_tree, n_estimators=20, n_jobs=4)\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
    "print(f\"MSE on train set: {mean_squared_error(y_train, bag.predict(X_train)):.2f}\")\n",
    "print(f\"MSE on test set: {mean_squared_error(y_test, bag.predict(X_test)):.2f}\")\n",
    "\n",
    "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке\n",
    "avg_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "    bag,\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    np.array(X_test),\n",
    "    np.array(y_test),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "print(f\"Average test loss: {avg_loss:.2f}\")\n",
    "print(f\"Average test bias: {avg_bias:.2f}\")\n",
    "print(f\"Average test variance: {avg_var:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-harvard",
   "metadata": {},
   "source": [
    "Как мы видим, по сравнению с единичным деревом смещение практически не изменилось, но дисперсия уменьшилась в несколько раз! Среднеквадратичная ошибка на тренировочной и тестовой выборках ближе, причем на тестовой уменьшилась, что говорит о том, что мы уменьшили переобучение единичного дерева.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-personal",
   "metadata": {},
   "source": [
    "### Случайный лес\n",
    "\n",
    "При построении каждого дерева в бэггинге в ходе создания очередного узла будем выбирать случайный набор признаков, на основе которых производится разбиение. В результате такой процедуры мы уменьшим корреляцию между деревьями, за счёт чего снизим дисперсию итоговой модели. Такой алгоритм назвывается **случайным лесом** (Random Forest).\n",
    "\n",
    "По сравнению с единичным деревом к параметрам случайного леса добавляются:\n",
    "- `max_features` – число признаков, на основе которых проводятся разбиения при построении дерева.\n",
    "- `n_estimators` – число деревьев.\n",
    "\n",
    "\n",
    "Вопрос: Как можно задать `max_features`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-bundle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:51:54.113083Z",
     "start_time": "2023-05-10T11:50:53.876014Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# TODO: обучите случайный лес с 20 деревьями, каждое из которых строится без ограничений\n",
    "rf = RandomForestRegressor(n_estimators=20, n_jobs=4)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n",
    "print(f\"MSE on train set: {mean_squared_error(y_train, rf.predict(X_train)):.2f}\")\n",
    "print(f\"MSE on train set: {mean_squared_error(y_test, rf.predict(X_test)):.2f}\")\n",
    "\n",
    "# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке\n",
    "avg_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "    rf,\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    np.array(X_test),\n",
    "    np.array(y_test),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "print(f\"Average test loss: {avg_loss:.2f}\")\n",
    "print(f\"Average test bias: {avg_bias:.2f}\")\n",
    "print(f\"Average test variance: {avg_var:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-portuguese",
   "metadata": {},
   "source": [
    "Как мы видим, по сравнению с простым бэггингом смещение вновь осталось практически неизменным, а дисперсия немного уменьшилась. Конечно, если подобрать хорошие гиперпараметры, то получится снизить дисперсию ещё больше. \n",
    "\n",
    "Ошибка на тренировочной выборке увеличилась, а на тестовой — уменьшилась, что означает, что мы добились нашей цели в борьбе с переобученными деревьями!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c6af5-1af4-4ce8-9cbf-c4459186331e",
   "metadata": {},
   "source": [
    "**Дополнительное задание:** Постройте какую-нибудь несложную функцию и постройте на одном графике предсказания деревьев, обученных на выборках с добавленым шумом для такой функции.\n",
    "Далее постройте такой же график, но для бэггингов, обученных на подвыборках. Для сравнения число семплов, которое вы выбирали для одного дерева (например, 50), должно остаться таким же для одного дерева в бэггинге. Для обучения бэггинга используйте больше семплов (50*n_trees). \n",
    "Ожидаемо вы увидете, что среднее предсказаний будет таким же (тот же Bias), но отдельные предсказания станут ближе к реальной зависимости (меньше Var).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-appeal",
   "metadata": {},
   "source": [
    "## Особенности случайного леса\n",
    "Много важных свойств леса описал в своём [блоге](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks) Лео Бриман (Leo Breiman), создатель случайного леса. Рассмотрим часть из них.\n",
    "Дополнительно: [статья от основ к практике](https://arxiv.org/pdf/1407.7502)\n",
    "### Число деревьев и \"Случайный лес не переобучается\"\n",
    "Cлучайный лес не переобучается именно с ростом числа деревьев (за счёт совместной работы бэггинга и использования случайных подпространств), но не в принципе. Посмотрим на поведение случайного леса при росте числа деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-threshold",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:35.735267Z",
     "start_time": "2023-05-10T11:51:54.114270Z"
    }
   },
   "outputs": [],
   "source": [
    "n_trees = 100\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for i in trange(1, n_trees, 2):\n",
    "    rf = RandomForestRegressor(n_estimators=i, random_state=123, n_jobs=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n",
    "    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Зависимость MSE от количества деревьев\")\n",
    "plt.grid()\n",
    "plt.plot(train_loss, label=\"MSE_train\")\n",
    "plt.plot(test_loss, label=\"MSE_test\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-swiss",
   "metadata": {},
   "source": [
    "Как и ожидалось, по достижении некоторого числа деревьев обе ошибки практически не изменяются, то есть дальнейшего переобучения при росте числа деревьев не происходит. Поэтому на практике основное ограничение -бюджет на обучение.\n",
    "\n",
    "При изменении какого-нибудь другого параметра на реальных данных переобучение может произойти. Например, случайный лес с ограниченными по глубине деревьями может предсказывать более точно, чем лес без ограничений. Примеры: [пример 1](https://datascience.stackexchange.com/questions/1028/do-random-forest-overfit), [пример 2](https://mljar.com/blog/random-forest-overfitting/).\n",
    "Посмотрим, какой будет ошибка в зависимости от глубины."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d717c-8138-4bc2-92bf-4a77e2ab1885",
   "metadata": {},
   "source": [
    "**Задание**: Постройте график зависимости от глубины. Наблюдается ли переобучение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-wilson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:43.149372Z",
     "start_time": "2023-05-10T11:52:35.737047Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = 50\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "################\n",
    "# YOUR CODE\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db3de75-ff81-4d44-9309-ccabb1efdd30",
   "metadata": {},
   "source": [
    "**Задание**: Постройте график зависимости от числа признаков в сплите (max_features). Наблюдается ли переобучение?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf6b24-e59e-4b4e-8611-fba429d74f7b",
   "metadata": {},
   "source": [
    "Вопрос: Как думаете, какую глубину чтоит выбирать? А если вспомнить о bias и variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-mississippi",
   "metadata": {},
   "source": [
    "### 3.2. Out-of-bag-ошибка\n",
    "\n",
    "Как мы обсудили выше, при построении случайного леса каждое дерево строится на бутстрапированной подвыборке, полученной из исходной обучающей выборки случайным набором с повторениями. В каждую подвыборку, таким образом, часть наблюдений попадет несколько раз, а часть ни разу. \n",
    "\n",
    "Какая вероятность, что семпл не попадет в подвыборку? Пусть в выборке $n$ наблюдений. Тогда вероятность семпла попасть в выборку с повторениями 1 раз - $1/n$. Не попасть - $1 - 1/n$. Бутстрап строит подвыборку размера n. Вероятность не попасть в подвыборку n раз тогда - $(1 - 1/n)^n$. Оценим асимптотику. При $ \\lim_{n -> \\inf}{(1 - 1/n)^n}$ сводится к замечательному пределу $(1 + 1/u)^u -> e $. \n",
    "В результате получаем $1/e \\approx 0.37$.\n",
    "\n",
    "А это значит, что мы можем получить оценку качества модели **бесплатно**! В случае MSE мы можем использовать MSE, а точности - error rate. Такая оценка называется **out-of-bag-ошибкой**. \n",
    "\n",
    "То, как конкретно она считается, зависит от реализации, но в целом алгоритм следующий: \n",
    "- Для каждого out-of-bag примера находим все модели, которые на нем не обучались.\n",
    "- Голосованием определяем значение предсказание этих моделей. Считаем соответствующую оценку\n",
    "- Считаем среднюю оценку на всех таких семплах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-california",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:44.771132Z",
     "start_time": "2023-05-10T11:52:43.162809Z"
    }
   },
   "outputs": [],
   "source": [
    "# oob_score_ = R2 на невиденных наблюдениях\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=123, oob_score=True, n_jobs=4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70cca8-fd3e-488e-a567-6066cda9d6c4",
   "metadata": {},
   "source": [
    "Вопрос: как OOB ошибка связана с оценкой на кросс-валидации?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa63791-cb0f-417a-b3f5-d89988d06fae",
   "metadata": {},
   "source": [
    "Важно: при небольшом количестве деревьев оценка будет неточной. \n",
    "\n",
    "**Задание:** постройте график зависимости OOB ошибки для количества деревьев и их глубины."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-evening",
   "metadata": {},
   "source": [
    "### 3.3. Важность признаков\n",
    "Как и решающие деревья, случайный лес позволяет оценивать важность признаков. Важность признаков можно оценивать, например, с помощью\n",
    "**Gini Importance**.\n",
    "Для одного дерева: при каждом разбиении по некоторому признаку, считаем Impurity decrease, взвешиваем по числу семплов в сплите и сохраняем результат. В конце суммируем.\n",
    "Лес расширяет эту оценку: считается среднее важности признака по всем деревьям.\n",
    "Однако тогда мы сталкиваемся с такой проблемой, что приоритет отдается признакам с большим числом уровней.\n",
    "\n",
    "Более правильный способ (но и более затратный) -  **Permurtation importance**.\n",
    "Для каждого дерева берется OOB семпл и считается количество голосов, отданных за правильный класс. Далее случайным образом перемешиваются значения переменной m в OOB и количество голосов, отданных за правильный класс, пересчитиывается.Далее одно вычитается из другого. Среднее  значение этой разницы по всем деревьям в лесу является необработанной оценкой важности для переменной m. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-monitor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:44.862097Z",
     "start_time": "2023-05-10T11:52:44.778753Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(california[\"feature_names\"], rf.feature_importances_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-breeding",
   "metadata": {},
   "source": [
    "Однако такие оценки опасны в случае коррелирующих признаков. Посмотрим, что произойдёт с важностью, если добавить в выборку линейно зависимый признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-burner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:44.865208Z",
     "start_time": "2023-05-10T11:52:44.863222Z"
    }
   },
   "outputs": [],
   "source": [
    "RM_mc = np.array((X_train.iloc[:, 5] * 2 + 3)).reshape(-1, 1)\n",
    "X_train_new = np.hstack((X_train, RM_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-slovakia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:46.662664Z",
     "start_time": "2023-05-10T11:52:44.866029Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-mechanism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:52:46.743774Z",
     "start_time": "2023-05-10T11:52:46.664943Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "names = list(california[\"feature_names\"])\n",
    "names.append(\"_AveOccup\")\n",
    "plt.bar(names, rf.feature_importances_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-absence",
   "metadata": {},
   "source": [
    "Важности перераспределились между линейной зависимыми признаками `AveOccup` и `_AveOccup`.  Поэтому такая оценка не обязательно четко отражает действительность. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64a528-4e49-47a3-94fd-06a5e66e8d6b",
   "metadata": {},
   "source": [
    "**Задание**: Посчитайте важности с помощью permutation importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977992b-576c-4c64-ad94-32330209aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff91ad9-332b-4829-839d-29fb87a9f860",
   "metadata": {},
   "source": [
    "## Что еще\n",
    "\n",
    "В библиотеке scikit-learn есть реализация ExtraTreesClassifier и ExtraTreesRegressor. Данный метод стоит использовать, если вы наблюдаете сильное переобучение на случайном лесе или градиентном бустинге.\n",
    "\n",
    "Случайный лес не очень хорошо работает для сильно разреженных данных.\n",
    "\n",
    "Как и одно дерево, RF не умеет экстраполировать.\n",
    "\n",
    "Для данных, включающих категориальные переменные с различным количеством уровней, случайные леса предвзяты в пользу признаков с большим количеством уровней: когда у признака много уровней, дерево будет сильнее подстраиваться именно под эти признаки, так как на них можно получить более высокое значение информативности\n",
    "\n",
    "RF хорошо параллелизуются, поэтому можно ускорить их обучение )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf614468-52be-4513-9855-e7c661d2aed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_2025",
   "language": "python",
   "name": "ml_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
